loss	accuracy	precision	recall	dataset	epoch
0.45305779576301575	0.9177946448326111	0.9177946448326111	0.9177946448326111	train	0
0.1521369069814682	0.9654765725135803	0.9654765725135803	0.9654765725135803	train	1
0.11883848905563354	0.9774088859558105	0.9774088859558105	0.9774088859558105	train	2
0.10137958079576492	0.9827383160591125	0.9827383160591125	0.9827383160591125	train	3
0.09056420624256134	0.9846720099449158	0.9846720099449158	0.9846720099449158	train	4
0.08127081394195557	0.9869357943534851	0.9869357943534851	0.9869357943534851	train	5
0.07552891224622726	0.9887751936912537	0.9887751936912537	0.9887751936912537	train	6
0.0691130980849266	0.9894826412200928	0.9894826412200928	0.9894826412200928	train	7
0.06561682373285294	0.9901900887489319	0.9901900887489319	0.9901900887489319	train	8
0.062478478997945786	0.9907088875770569	0.9907088875770569	0.9907088875770569	train	9
0.059418655931949615	0.9913691282272339	0.9913691282272339	0.9913691282272339	train	10
0.05676468089222908	0.992595374584198	0.992595374584198	0.992595374584198	train	11
0.05499998852610588	0.9925482273101807	0.9925482273101807	0.9925482273101807	train	12
0.052644193172454834	0.9933499693870544	0.9933499693870544	0.9933499693870544	train	13
0.05152418464422226	0.9935858249664307	0.9935858249664307	0.9935858249664307	train	14
0.048514001071453094	0.9940102696418762	0.9940102696418762	0.9940102696418762	train	15
0.048693154007196426	0.9933499693870544	0.9933499693870544	0.9933499693870544	train	16
0.04733022674918175	0.9933499693870544	0.9933499693870544	0.9933499693870544	train	17
0.0457204170525074	0.9935858249664307	0.9935858249664307	0.9935858249664307	train	18
0.04510191082954407	0.9940574169158936	0.9940574169158936	0.9940574169158936	train	19
0.04327569156885147	0.9945290684700012	0.9945290684700012	0.9945290684700012	train	20
0.04329532012343407	0.9941989183425903	0.9941989183425903	0.9941989183425903	train	21
0.04119943827390671	0.9948120713233948	0.9948120713233948	0.9948120713233948	train	22
0.21307101845741272	0.9656668305397034	0.9656668305397034	0.9656668305397034	validation	0
0.17165613174438477	0.9820788502693176	0.9820788502693176	0.9820788502693176	validation	1
0.13671770691871643	0.9858517050743103	0.9858517050743103	0.9858517050743103	validation	2
0.12788285315036774	0.9879267811775208	0.9879267811775208	0.9879267811775208	validation	3
0.12043935805559158	0.9845312237739563	0.9845312237739563	0.9845312237739563	validation	4
0.10209901630878448	0.9901905059814453	0.9901905059814453	0.9901905059814453	validation	5
0.0991109237074852	0.9903791546821594	0.9903791546821594	0.9903791546821594	validation	6
0.09107799082994461	0.9920769929885864	0.9920769929885864	0.9920769929885864	validation	7
0.09495938569307327	0.9892473220825195	0.9892473220825195	0.9892473220825195	validation	8
0.08397164940834045	0.9933974742889404	0.9933974742889404	0.9933974742889404	validation	9
0.0825236588716507	0.9928315281867981	0.9928315281867981	0.9928315281867981	validation	10
0.08056575804948807	0.9937747716903687	0.9937747716903687	0.9937747716903687	validation	11
0.08800365775823593	0.988304078578949	0.988304078578949	0.988304078578949	validation	12
0.08755157887935638	0.9892473220825195	0.9892473220825195	0.9892473220825195	validation	13
0.0685625672340393	0.9950952529907227	0.9950952529907227	0.9950952529907227	validation	14
0.06778840720653534	0.9945293068885803	0.9945293068885803	0.9945293068885803	validation	15
0.07020413875579834	0.9947179555892944	0.9947179555892944	0.9947179555892944	validation	16
0.06273459643125534	0.995661199092865	0.995661199092865	0.995661199092865	validation	17
0.06760083884000778	0.9937747716903687	0.9937747716903687	0.9937747716903687	validation	18
0.06005381420254707	0.994340717792511	0.994340717792511	0.994340717792511	validation	19
0.06377891451120377	0.994340717792511	0.994340717792511	0.994340717792511	validation	20
0.06061560660600662	0.9950952529907227	0.9950952529907227	0.9950952529907227	validation	21
0.06787931174039841	0.9924542307853699	0.9924542307853699	0.9924542307853699	validation	22
