loss	accuracy	precision	recall	dataset	epoch
0.5159205198287964	0.8525413274765015	0.8525413274765015	0.8525413274765015	train	0
0.25027960538864136	0.9565857648849487	0.9565857648849487	0.9565857648849487	train	1
0.210957333445549	0.9629844427108765	0.9629844427108765	0.9629844427108765	train	2
0.19004464149475098	0.9674996733665466	0.9674996733665466	0.9674996733665466	train	3
0.17671656608581543	0.9691174626350403	0.9691174626350403	0.9691174626350403	train	4
0.1670251339673996	0.9701315760612488	0.9701315760612488	0.9701315760612488	train	5
0.15825465321540833	0.9712664484977722	0.9712664484977722	0.9712664484977722	train	6
0.15425141155719757	0.9710974097251892	0.9710974097251892	0.9710974097251892	train	7
0.15038274228572845	0.9710732698440552	0.9710732698440552	0.9710732698440552	train	8
0.14553524553775787	0.9726910591125488	0.9726910591125488	0.9726910591125488	train	9
0.1418287456035614	0.9736327528953552	0.9736327528953552	0.9736327528953552	train	10
0.1377853900194168	0.9736568927764893	0.9736568927764893	0.9736568927764893	train	11
0.1363460123538971	0.9738500714302063	0.9738500714302063	0.9738500714302063	train	12
0.13293524086475372	0.9739466309547424	0.9739466309547424	0.9739466309547424	train	13
0.1297965943813324	0.9753953814506531	0.9753953814506531	0.9753953814506531	train	14
0.1289401650428772	0.9751297831535339	0.9751297831535339	0.9751297831535339	train	15
0.12766695022583008	0.9747676253318787	0.9747676253318787	0.9747676253318787	train	16
0.12641699612140656	0.9744054079055786	0.9744054079055786	0.9744054079055786	train	17
0.12285908311605453	0.9747917652130127	0.9747917652130127	0.9747917652130127	train	18
0.12355918437242508	0.9754195213317871	0.9754195213317871	0.9754195213317871	train	19
0.1203279122710228	0.975153923034668	0.975153923034668	0.975153923034668	train	20
0.119418665766716	0.9759990572929382	0.9759990572929382	0.9759990572929382	train	21
0.11996963620185852	0.9760714769363403	0.9760714769363403	0.9760714769363403	train	22
0.11723680794239044	0.9763612151145935	0.9763612151145935	0.9763612151145935	train	23
0.11751437932252884	0.9759749174118042	0.9759749174118042	0.9759749174118042	train	24
0.11635316908359528	0.9759265780448914	0.9759265780448914	0.9759265780448914	train	25
0.11515691131353378	0.9761680364608765	0.9761680364608765	0.9761680364608765	train	26
0.11487795412540436	0.9764336347579956	0.9764336347579956	0.9764336347579956	train	27
0.11412665247917175	0.9765785336494446	0.9765785336494446	0.9765785336494446	train	28
0.11334511637687683	0.9766750931739807	0.9766750931739807	0.9766750931739807	train	29
0.11210669577121735	0.9762646555900574	0.9762646555900574	0.9762646555900574	train	30
0.11081232875585556	0.9770131707191467	0.9770131707191467	0.9770131707191467	train	31
0.10985002666711807	0.9767234325408936	0.9767234325408936	0.9767234325408936	train	32
0.11140027642250061	0.9770855903625488	0.9770855903625488	0.9770855903625488	train	33
0.26308658719062805	0.9599188566207886	0.9599188566207886	0.9599188566207886	validation	0
0.20446854829788208	0.9673556089401245	0.9673556089401245	0.9673556089401245	validation	1
0.17772744596004486	0.9746957421302795	0.9746957421302795	0.9746957421302795	validation	2
0.16957256197929382	0.9729573130607605	0.9729573130607605	0.9729573130607605	validation	3
0.14998863637447357	0.9761444926261902	0.9761444926261902	0.9761444926261902	validation	4
0.14541034400463104	0.9770137071609497	0.9770137071609497	0.9770137071609497	validation	5
0.139020636677742	0.976047933101654	0.976047933101654	0.976047933101654	validation	6
0.13336427509784698	0.9769171476364136	0.9769171476364136	0.9769171476364136	validation	7
0.12899751961231232	0.9767239689826965	0.9767239689826965	0.9767239689826965	validation	8
0.1279853880405426	0.9774966239929199	0.9774966239929199	0.9774966239929199	validation	9
0.12482138723134995	0.9770137071609497	0.9770137071609497	0.9770137071609497	validation	10
0.11993839591741562	0.9781726598739624	0.9781726598739624	0.9781726598739624	validation	11
0.11832655966281891	0.9783658385276794	0.9783658385276794	0.9783658385276794	validation	12
0.12027952075004578	0.9767239689826965	0.9767239689826965	0.9767239689826965	validation	13
0.11324385553598404	0.9780761003494263	0.9780761003494263	0.9780761003494263	validation	14
0.11173974722623825	0.9785590171813965	0.9785590171813965	0.9785590171813965	validation	15
0.1116815060377121	0.9773034453392029	0.9773034453392029	0.9773034453392029	validation	16
0.10938113182783127	0.9782692790031433	0.9782692790031433	0.9782692790031433	validation	17
0.10793306678533554	0.9788487553596497	0.9788487553596497	0.9788487553596497	validation	18
0.10683389008045197	0.9780761003494263	0.9780761003494263	0.9780761003494263	validation	19
0.10458634793758392	0.9788487553596497	0.9788487553596497	0.9788487553596497	validation	20
0.1054529994726181	0.979235053062439	0.979235053062439	0.979235053062439	validation	21
0.10343300551176071	0.9789453148841858	0.9789453148841858	0.9789453148841858	validation	22
0.10302381962537766	0.9793316721916199	0.9793316721916199	0.9793316721916199	validation	23
0.10389033704996109	0.9784624576568604	0.9784624576568604	0.9784624576568604	validation	24
0.10142439603805542	0.9795248508453369	0.9795248508453369	0.9795248508453369	validation	25
0.10288193076848984	0.979621410369873	0.979621410369873	0.979621410369873	validation	26
0.10249147564172745	0.9784624576568604	0.9784624576568604	0.9784624576568604	validation	27
0.09813529998064041	0.979621410369873	0.979621410369873	0.979621410369873	validation	28
0.09821468591690063	0.979235053062439	0.979235053062439	0.979235053062439	validation	29
0.0968656837940216	0.9797179698944092	0.9797179698944092	0.9797179698944092	validation	30
0.09633989632129669	0.9802974462509155	0.9802974462509155	0.9802974462509155	validation	31
0.09611903876066208	0.9793316721916199	0.9793316721916199	0.9793316721916199	validation	32
0.09678168594837189	0.979428231716156	0.979428231716156	0.979428231716156	validation	33
