loss	accuracy	precision	recall	dataset	epoch
0.8720310926437378	0.7109248042106628	0.7109248042106628	0.7109248042106628	train	0
0.3625630736351013	0.9213088154792786	0.9213088154792786	0.9213088154792786	train	1
0.2793697416782379	0.9542320370674133	0.9542320370674133	0.9542320370674133	train	2
0.23899635672569275	0.9639670252799988	0.9639670252799988	0.9639670252799988	train	3
0.21688956022262573	0.968090832233429	0.968090832233429	0.968090832233429	train	4
0.19744811952114105	0.9735667705535889	0.9735667705535889	0.9735667705535889	train	5
0.18601657450199127	0.974175214767456	0.974175214767456	0.974175214767456	train	6
0.1771649718284607	0.9762709736824036	0.9762709736824036	0.9762709736824036	train	7
0.16556033492088318	0.9766089916229248	0.9766089916229248	0.9766089916229248	train	8
0.15937235951423645	0.9775554537773132	0.9775554537773132	0.9775554537773132	train	9
0.15416660904884338	0.9785019159317017	0.9785019159317017	0.9785019159317017	train	10
0.14687636494636536	0.9794483780860901	0.9794483780860901	0.9794483780860901	train	11
0.14509208500385284	0.9801244139671326	0.9801244139671326	0.9801244139671326	train	12
0.14195433259010315	0.9806652069091797	0.9806652069091797	0.9806652069091797	train	13
0.1378306895494461	0.9801244139671326	0.9801244139671326	0.9801244139671326	train	14
0.1338495910167694	0.9818145036697388	0.9818145036697388	0.9818145036697388	train	15
0.1316959112882614	0.9814088940620422	0.9814088940620422	0.9814088940620422	train	16
0.12877261638641357	0.9826933741569519	0.9826933741569519	0.9826933741569519	train	17
0.12540099024772644	0.9826257228851318	0.9826257228851318	0.9826257228851318	train	18
0.12087056040763855	0.9839778542518616	0.9839778542518616	0.9839778542518616	train	19
0.12130612879991531	0.9822201132774353	0.9822201132774353	0.9822201132774353	train	20
0.11767924576997757	0.9844510555267334	0.9844510555267334	0.9844510555267334	train	21
0.1174580380320549	0.9843158721923828	0.9843158721923828	0.9843158721923828	train	22
0.11599981784820557	0.9830313920974731	0.9830313920974731	0.9830313920974731	train	23
0.11469235271215439	0.9837074279785156	0.9837074279785156	0.9837074279785156	train	24
0.11023057252168655	0.9853975176811218	0.9853975176811218	0.9853975176811218	train	25
0.11023545265197754	0.9843834638595581	0.9843834638595581	0.9843834638595581	train	26
0.10780979692935944	0.9851270914077759	0.9851270914077759	0.9851270914077759	train	27
0.1063709557056427	0.9854651093482971	0.9854651093482971	0.9854651093482971	train	28
0.10564129054546356	0.9859383702278137	0.9859383702278137	0.9859383702278137	train	29
0.10476915538311005	0.9853299260139465	0.9853299260139465	0.9853299260139465	train	30
0.10577817261219025	0.9848566651344299	0.9848566651344299	0.9848566651344299	train	31
0.10048027336597443	0.986276388168335	0.986276388168335	0.986276388168335	train	32
0.10117007046937943	0.9856679439544678	0.9856679439544678	0.9856679439544678	train	33
0.10042237490415573	0.9858707189559937	0.9858707189559937	0.9858707189559937	train	34
0.10103874653577805	0.9862087368965149	0.9862087368965149	0.9862087368965149	train	35
0.09788680821657181	0.9874932169914246	0.9874932169914246	0.9874932169914246	train	36
0.09841800481081009	0.9863439798355103	0.9863439798355103	0.9863439798355103	train	37
0.09499606490135193	0.9874932169914246	0.9874932169914246	0.9874932169914246	train	38
0.0948936864733696	0.9874256253242493	0.9874256253242493	0.9874256253242493	train	39
0.09610266983509064	0.9857355356216431	0.9857355356216431	0.9857355356216431	train	40
0.42776259779930115	0.896999180316925	0.896999180316925	0.896999180316925	validation	0
0.27605926990509033	0.9751284122467041	0.9751284122467041	0.9751284122467041	validation	1
0.23192216455936432	0.974047064781189	0.974047064781189	0.974047064781189	validation	2
0.21078668534755707	0.977020800113678	0.977020800113678	0.977020800113678	validation	3
0.18152718245983124	0.9851311445236206	0.9851311445236206	0.9851311445236206	validation	4
0.16886641085147858	0.9821573495864868	0.9821573495864868	0.9821573495864868	validation	5
0.1595434695482254	0.9872938394546509	0.9872938394546509	0.9872938394546509	validation	6
0.1551939994096756	0.9813463091850281	0.9813463091850281	0.9813463091850281	validation	7
0.14282406866550446	0.9889159202575684	0.9889159202575684	0.9889159202575684	validation	8
0.13821564614772797	0.9883752465248108	0.9883752465248108	0.9883752465248108	validation	9
0.13499195873737335	0.9886455535888672	0.9886455535888672	0.9886455535888672	validation	10
0.1290597915649414	0.9872938394546509	0.9872938394546509	0.9872938394546509	validation	11
0.12315063923597336	0.9883752465248108	0.9883752465248108	0.9883752465248108	validation	12
0.12920866906642914	0.9872938394546509	0.9872938394546509	0.9872938394546509	validation	13
0.11684446036815643	0.9902676343917847	0.9902676343917847	0.9902676343917847	validation	14
0.11664232611656189	0.9889159202575684	0.9889159202575684	0.9889159202575684	validation	15
0.11738868057727814	0.9889159202575684	0.9889159202575684	0.9889159202575684	validation	16
0.11030254513025284	0.9905380010604858	0.9905380010604858	0.9905380010604858	validation	17
0.10932208597660065	0.9899972677230835	0.9899972677230835	0.9899972677230835	validation	18
0.10625957697629929	0.9908083081245422	0.9908083081245422	0.9908083081245422	validation	19
0.10455406457185745	0.9905380010604858	0.9905380010604858	0.9905380010604858	validation	20
0.10274174809455872	0.9910786747932434	0.9910786747932434	0.9910786747932434	validation	21
0.10176519304513931	0.9905380010604858	0.9905380010604858	0.9905380010604858	validation	22
0.09920601546764374	0.9902676343917847	0.9902676343917847	0.9902676343917847	validation	23
0.10086343437433243	0.9921600222587585	0.9921600222587585	0.9921600222587585	validation	24
0.09438013285398483	0.9910786747932434	0.9910786747932434	0.9910786747932434	validation	25
0.0934319719672203	0.9910786747932434	0.9910786747932434	0.9910786747932434	validation	26
0.09148773550987244	0.9905380010604858	0.9905380010604858	0.9905380010604858	validation	27
0.09048712253570557	0.9913490414619446	0.9913490414619446	0.9913490414619446	validation	28
0.09209743142127991	0.9908083081245422	0.9908083081245422	0.9908083081245422	validation	29
0.09205574542284012	0.9921600222587585	0.9921600222587585	0.9921600222587585	validation	30
0.0877285972237587	0.9913490414619446	0.9913490414619446	0.9913490414619446	validation	31
0.08901050686836243	0.991619348526001	0.991619348526001	0.991619348526001	validation	32
0.08720673620700836	0.9918897151947021	0.9918897151947021	0.9918897151947021	validation	33
0.08492064476013184	0.991619348526001	0.991619348526001	0.991619348526001	validation	34
0.08597055077552795	0.9910786747932434	0.9910786747932434	0.9910786747932434	validation	35
0.0856112390756607	0.9894565939903259	0.9894565939903259	0.9894565939903259	validation	36
0.08309994637966156	0.9921600222587585	0.9921600222587585	0.9921600222587585	validation	37
0.08294844627380371	0.9918897151947021	0.9918897151947021	0.9918897151947021	validation	38
0.08289936929941177	0.9924303889274597	0.9924303889274597	0.9924303889274597	validation	39
0.08305417001247406	0.9921600222587585	0.9921600222587585	0.9921600222587585	validation	40
