loss	accuracy	precision	recall	dataset	epoch
0.5257501006126404	0.8444899320602417	0.8444899320602417	0.8444899320602417	train	0
0.24664199352264404	0.9584442377090454	0.9584442377090454	0.9584442377090454	train	1
0.20626714825630188	0.9649266600608826	0.9649266600608826	0.9649266600608826	train	2
0.1863068789243698	0.9678835272789001	0.9678835272789001	0.9678835272789001	train	3
0.17384746670722961	0.9683839678764343	0.9683839678764343	0.9683839678764343	train	4
0.16499094665050507	0.9715228080749512	0.9715228080749512	0.9715228080749512	train	5
0.15609407424926758	0.9719549417495728	0.9719549417495728	0.9719549417495728	train	6
0.15210066735744476	0.9721823930740356	0.9721823930740356	0.9721823930740356	train	7
0.1471325010061264	0.9718412160873413	0.9718412160873413	0.9718412160873413	train	8
0.14488276839256287	0.9721823930740356	0.9721823930740356	0.9721823930740356	train	9
0.1401485949754715	0.973251461982727	0.973251461982727	0.973251461982727	train	10
0.13667233288288116	0.9728875160217285	0.9728875160217285	0.9728875160217285	train	11
0.1337212771177292	0.974115788936615	0.974115788936615	0.974115788936615	train	12
0.1323116570711136	0.9736608862876892	0.9736608862876892	0.9736608862876892	train	13
0.1288883537054062	0.9750483632087708	0.9750483632087708	0.9750483632087708	train	14
0.12699568271636963	0.9746844172477722	0.9746844172477722	0.9746844172477722	train	15
0.1266801953315735	0.9743204712867737	0.9743204712867737	0.9743204712867737	train	16
0.12306389212608337	0.9752758145332336	0.9752758145332336	0.9752758145332336	train	17
0.12186723947525024	0.9754350185394287	0.9754350185394287	0.9754350185394287	train	18
0.12128701061010361	0.9742067456245422	0.9742067456245422	0.9742067456245422	train	19
0.12106529623270035	0.9748890995979309	0.9748890995979309	0.9748890995979309	train	20
0.11998017132282257	0.9754804968833923	0.9754804968833923	0.9754804968833923	train	21
0.11762098222970963	0.9758444428443909	0.9758444428443909	0.9758444428443909	train	22
0.11750372499227524	0.9752985239028931	0.9752985239028931	0.9752985239028931	train	23
0.11656905710697174	0.9757534265518188	0.9757534265518188	0.9757534265518188	train	24
0.11603865027427673	0.9757989048957825	0.9757989048957825	0.9757989048957825	train	25
0.1141471415758133	0.975776195526123	0.975776195526123	0.975776195526123	train	26
0.1130705252289772	0.9773683547973633	0.9773683547973633	0.9773683547973633	train	27
0.11272592842578888	0.9766177535057068	0.9766177535057068	0.9766177535057068	train	28
0.259764701128006	0.9677037596702576	0.9677037596702576	0.9677037596702576	validation	0
0.19942422211170197	0.9703420400619507	0.9703420400619507	0.9703420400619507	validation	1
0.17510350048542023	0.9723435044288635	0.9723435044288635	0.9723435044288635	validation	2
0.15937045216560364	0.9766193628311157	0.9766193628311157	0.9766193628311157	validation	3
0.15325696766376495	0.9776200652122498	0.9776200652122498	0.9776200652122498	validation	4
0.140391543507576	0.9793485999107361	0.9793485999107361	0.9793485999107361	validation	5
0.13403338193893433	0.9779840111732483	0.9779840111732483	0.9779840111732483	validation	6
0.1273878812789917	0.978347897529602	0.978347897529602	0.978347897529602	validation	7
0.1253678798675537	0.9785298109054565	0.9785298109054565	0.9785298109054565	validation	8
0.12129140645265579	0.9781659245491028	0.9781659245491028	0.9781659245491028	validation	9
0.12058550119400024	0.978347897529602	0.978347897529602	0.978347897529602	validation	10
0.11606715619564056	0.9798944592475891	0.9798944592475891	0.9798944592475891	validation	11
0.11513619124889374	0.9784388542175293	0.9784388542175293	0.9784388542175293	validation	12
0.11231502890586853	0.9803493618965149	0.9803493618965149	0.9803493618965149	validation	13
0.10835999250411987	0.9793485999107361	0.9793485999107361	0.9793485999107361	validation	14
0.10702571272850037	0.9794396162033081	0.9794396162033081	0.9794396162033081	validation	15
0.10661786794662476	0.9804403185844421	0.9804403185844421	0.9804403185844421	validation	16
0.10719674825668335	0.9800764322280884	0.9800764322280884	0.9800764322280884	validation	17
0.10448573529720306	0.9800764322280884	0.9800764322280884	0.9800764322280884	validation	18
0.10444165766239166	0.9797125458717346	0.9797125458717346	0.9797125458717346	validation	19
0.10318496078252792	0.9798944592475891	0.9798944592475891	0.9798944592475891	validation	20
0.10056369006633759	0.9796215295791626	0.9796215295791626	0.9796215295791626	validation	21
0.10229931026697159	0.9802583456039429	0.9802583456039429	0.9802583456039429	validation	22
0.09939659386873245	0.9802583456039429	0.9802583456039429	0.9802583456039429	validation	23
0.09886565804481506	0.9804403185844421	0.9804403185844421	0.9804403185844421	validation	24
0.09733609110116959	0.9802583456039429	0.9802583456039429	0.9802583456039429	validation	25
0.1013905256986618	0.9795305728912354	0.9795305728912354	0.9795305728912354	validation	26
0.09907728433609009	0.9803493618965149	0.9803493618965149	0.9803493618965149	validation	27
0.09930494427680969	0.9808952212333679	0.9808952212333679	0.9808952212333679	validation	28
