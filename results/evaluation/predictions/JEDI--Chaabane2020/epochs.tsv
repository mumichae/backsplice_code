epoch	loss	acc	pre	f1	mcc	sen	spe	dataset
1	0.3590448200702667	0.8525447306566892	0.8347938144329897	0.6781825795644891	0.6046637478658239	0.5710507757404796	0.9577594728171335	train
2	0.23862454295158386	0.9054060536288195	0.8472222222222222	0.8207272727272727	0.7572485891786342	0.7958392101551481	0.9463591433278419	train
3	0.22934341430664062	0.9071808893365952	0.8503656478529908	0.8241708314402545	0.761867209825721	0.7995416078984485	0.9474135090609556	train
4	0.22567231953144073	0.9082361970547321	0.8556291390728477	0.8254084147120562	0.7641527427424976	0.7972496473906912	0.9497199341021417	train
5	0.2263825237751007	0.9061255816184582	0.8492197781537884	0.8219452279137476	0.759016498145612	0.7963681241184767	0.9471499176276771	train
6	0.22379560768604279	0.9083321341200172	0.8579859128117266	0.8250800915331807	0.7641273810701147	0.794605077574048	0.950840197693575	train
7	0.2239743322134018	0.9080922914568044	0.8568985176738882	0.8247667825132613	0.7635827055430868	0.7949576868829337	0.9503789126853377	train
8	0.22193428874015808	0.9080922914568044	0.85527809307605	0.825150574922431	0.7637898410823991	0.7970733427362482	0.9495881383855025	train
9	0.22242210805416107	0.9072768264018803	0.8556210766596919	0.8231311190410834	0.7614305833160804	0.7930183356840621	0.9499835255354201	train
10	0.22236984968185425	0.9078524487935914	0.8571700628451724	0.8241325643138332	0.762869174180227	0.7935472496473907	0.9505766062602965	train
11	0.2234688550233841	0.906509329879599	0.8539646320593268	0.8216997529960661	0.7594623219315473	0.7917842031029619	0.9493904448105437	train
12	0.22155573964118958	0.9089077565117283	0.8602253198396028	0.8259235493629113	0.7654862950945379	0.7942524682651622	0.9517627677100494	train
13	0.22170068323612213	0.9088118194464432	0.8619696678825111	0.8252917930337285	0.7650156103213144	0.7916078984485191	0.9526194398682043	train
14	0.22220569849014282	0.9069890152060248	0.8545109211775879	0.8227118954009326	0.7607552674745159	0.793194640338505	0.9495222405271828	train
15	0.21985982358455658	0.9088597879790857	0.8603362628964463	0.8257839721254355	0.7653378241785461	0.7938998589562765	0.951828665568369	train
16	0.22045622766017914	0.9090996306422987	0.8599199542595769	0.8264493085447385	0.7660643875254469	0.7954866008462623	0.9515650741350906	train
17	0.21968553960323334	0.9085240082505877	0.8580939699448354	0.8255101107146126	0.7646573117168931	0.7953102961918195	0.950840197693575	train
18	0.21941202878952026	0.9095313474360819	0.8645993836671803	0.8263991163475699	0.7667544295847343	0.7914315937940761	0.953673805601318	train
1	0.2676714062690735	0.898512675660154	0.8371912523398621	0.8089912425826661	0.7407758700315489	0.7826291079812207	0.9423825758187782	validation
2	0.2403493970632553	0.9028140785972972	0.8782230661603038	0.8091248100916164	0.7486406474312826	0.7501067008109261	0.9606243234072805	validation
3	0.23335817456245422	0.9022983790625989	0.840445687477445	0.8171849642528181	0.7511257307172319	0.7951771233461374	0.9428511415230001	validation
4	0.23614908754825592	0.8990400956388228	0.8078969243557772	0.8186144451463466	0.7488119161253678	0.8296201451131029	0.925320321209869	validation
5	0.23044420778751373	0.9021811746228947	0.8535202024936721	0.8135555356983291	0.7489580885789302	0.7771660264618011	0.949508006010567	validation
6	0.23127657175064087	0.9023452608384805	0.8383684446436576	0.8178399650196765	0.7516067996683349	0.7982927870251814	0.9417362782957134	validation
7	0.23050978779792786	0.9018881635236343	0.8301977809937289	0.8189388532000952	0.7517997592218674	0.807981220657277	0.9374383997673329	validation
8	0.23683932423591614	0.8995675156174916	0.8089733460850763	0.8195202089344764	0.7500844884897336	0.8303457106274008	0.9257727294760143	validation
9	0.23132848739624023	0.9033883803518478	0.868885110517367	0.8127229353629445	0.7508377294108334	0.7633802816901408	0.9563910746312064	validation
10	0.23073066771030426	0.9035993483433152	0.8623171138540724	0.8148064755814739	0.7519985459137056	0.7722577891591976	0.9533211613966489	validation
11	0.2354785054922104	0.9029664443689127	0.8787939396969848	0.8093670128255313	0.7490307055673779	0.7501067008109261	0.9608343701022766	validation
12	0.23607367277145386	0.9031539714724394	0.8824449039285894	0.8089877250976676	0.7493728527977513	0.7468203158344003	0.9623370118434021	validation
13	0.23018716275691986	0.9026851537136227	0.8345053292645173	0.819660737169045	0.7532764153790661	0.8053350405463081	0.9395388667172933	validation
14	0.2306796759366989	0.9023218199505397	0.8339823008849557	0.818944166847708	0.75232367801869	0.8044387537345283	0.9393772923365271	validation
15	0.22918586432933807	0.9026148310498001	0.8468278361392725	0.816290433129187	0.751058275452557	0.7878787878787878	0.9460503142621706	validation
16	0.2300061136484146	0.9031188101405281	0.8613918017159199	0.8138707498311192	0.7507433881700863	0.7713188220230474	0.9530141700731932	validation
17	0.23249825835227966	0.9015599910924625	0.8791303031831711	0.8058169375534644	0.7451395301858772	0.7437900128040973	0.9612867783684219	validation
18	0.22976455092430115	0.9024273039462735	0.8449732791303156	0.8163184255234649	0.7507921583514757	0.789543320529236	0.9451616551679566	validation
